{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGkz0vzQmpRR",
        "outputId": "544062ce-66d3-4c1e-b29c-693f1f36805c"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install seqeval\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam, BertForSequenceClassification,BertModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIX3Hwh0m1-n"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "with open(\"corpus.txt\", 'w') as f:\n",
        "    \n",
        "    for token in tokenizer.vocab.keys():\n",
        "                 \n",
        "        f.write(token + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BGQLnijn2tP",
        "outputId": "5e5c485f-a0e6-4ecf-b483-4d438eca3707"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbZCeJk3qCAI",
        "outputId": "951bbcdc-ce14-4d02-97df-8f31cbae3924"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUMiEYneqF8n",
        "outputId": "7a3bbd9b-7b13-4cac-bd14-74fab9db9b65"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmzkfve-8OQM",
        "outputId": "b4ba0f65-4644-4962-ce53-189068815f4a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df=pd.read_csv('/content/drive/My Drive/tweets.csv',delimiter=',',encoding='ISO-8859-1',low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXCJjM2rknk0",
        "outputId": "9659c320-2255-4be1-e180-03dca74eaee3"
      },
      "outputs": [],
      "source": [
        "input_ids = []\n",
        "token_text = []\n",
        "tweetdf = df[0:10000]\n",
        "df2 = df[810000:820000]\n",
        "tweetdf = tweetdf.append(df2)\n",
        "tweetdf.columns = ['sentiment','id','date','query','uname','tweet']\n",
        "print(len(tweetdf))\n",
        "sentences = tweetdf.tweet.values\n",
        "labels = tweetdf.sentiment.values\n",
        "\n",
        "for sent in sentences:\n",
        "    tokenized_text = tokenizer.tokenize(sent)\n",
        "    token_text.append(tokenized_text)\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 512 \n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPSPQRBhnvz0",
        "outputId": "5260fc6b-4bbd-4581-e8c0-6df98ef5d02e"
      },
      "outputs": [],
      "source": [
        "print('Original: ', sentences[11])\n",
        "print('Labels: ', labels)\n",
        "print('Token IDs:', input_ids[11])\n",
        "print('tokenized text: ',token_text[11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFw2mHdy8OCC",
        "outputId": "66901411-9ac2-4289-ae95-1c48acc82d64"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSAqS-pD8N34"
      },
      "outputs": [],
      "source": [
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c44nWvy8Nqo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkUIiwWk8NJk",
        "outputId": "cc00d290-0816-48a4-ccae-7b8bfefd12aa"
      },
      "outputs": [],
      "source": [
        "train_inputs = torch.LongTensor(train_inputs)\n",
        "validation_inputs = torch.LongTensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.LongTensor(train_labels)\n",
        "validation_labels = torch.LongTensor(validation_labels)\n",
        "\n",
        "train_masks = torch.LongTensor(train_masks)\n",
        "validation_masks = torch.LongTensor(validation_masks)\n",
        "\n",
        "print(train_inputs[11])\n",
        "print(train_masks[11])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDBLtO1dKBq_",
        "outputId": "e87f1f8e-6630-4e8b-9694-b93a5b7085b9"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "#train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
        "\n",
        "\n",
        "\n",
        "valid_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=16)\n",
        "\n",
        "print(len(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "LQeZLrbhKePu",
        "outputId": "f1d0cbea-9239-4999-f86d-348a5282883b"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
        "model = model.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d3doJLEjbZf"
      },
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSefblvvjdxb"
      },
      "outputs": [],
      "source": [
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7e4PyLLjlgs"
      },
      "outputs": [],
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djItk4pibQnJ"
      },
      "outputs": [],
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "JT_kYh4fTLe8",
        "outputId": "a6b4a0f2-8b6f-4713-83fe-e770114eb5f4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "t = [] \n",
        "loss_ot = []\n",
        "accuracy_ot = []\n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "epochs = 5\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    # TRAIN loop\n",
        "      model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "      tr_loss = 0\n",
        "      nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        outputs = model(b_input_ids)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs,b_labels)\n",
        "        train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "        loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        #print(\"Iteration: {}/{} | Loss: {}\".format(step,len(train_dataloader),loss))\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "      loss_ot.append(tr_loss/nb_tr_steps)\n",
        "      #train_loss = tr_loss/nb_tr_steps\n",
        "      print(\"Training Loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "      #print(\"Epoch {}/{} | Train loss: {}\".format(epoch+1,epochs,train_loss))\n",
        "    \n",
        "    # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "      model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "      for batch in valid_dataloader:\n",
        "    # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "      accuracy_ot.append(eval_accuracy/nb_eval_steps)\n",
        "      #val_accuracy = eval_accuracy/nb_eval_steps\n",
        "    \n",
        "    \n",
        "    \n",
        "      #print(\"Train Loss: {} | Validation Accuracy: {}\".format(train_loss,val_accuracy))\n",
        "      print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "torch.save(model,\"/content/drive/My Drive/bertmodel1.pth\")\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "epochs_ot = list(range(0,epochs))\n",
        "# plotting the points  \n",
        "plt.plot(epochs_ot, accuracy_ot) \n",
        "  \n",
        "# naming the x axis \n",
        "plt.xlabel('Epochs') \n",
        "# naming the y axis \n",
        "plt.ylabel('Accuracy') \n",
        "  \n",
        "# giving a title to my graph \n",
        "plt.title('Epochs vs Accuracy!') \n",
        "  \n",
        "# function to show the plot \n",
        "plt.show() \n",
        "\n",
        "plt.plot(epochs_ot,loss_ot)\n",
        "plt.xlabel('Epochs') \n",
        "# naming the y axis \n",
        "plt.ylabel('Loss') \n",
        "  \n",
        "# giving a title to my graph \n",
        "plt.title('Epochs vs Loss!') \n",
        "  \n",
        "# function to show the plot \n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fucifa-Ho4DS",
        "outputId": "12c7448c-c464-4867-affc-cfdde75aed8f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model = torch.load(\"/content/drive/My Drive/bertmodel1.pth\")\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0r48AgHaI_S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=1)[:, None])\n",
        "    return e_x / np.sum(e_x, axis=1)[:, None]\n",
        "\n",
        "\n",
        "def keywordextract(text,filename):\n",
        "  tkns = tokenizer.tokenize(text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tkns)\n",
        "  segments_ids = [0] * len(tkns)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "  segments_tensors = torch.tensor([segments_ids]).to(device)\n",
        "  label_list = ['negative','positive']\n",
        "  label_dict = {0: 'negative' , 1: 'positive'}\n",
        "  result = pd.DataFrame(columns=['sentence','logit','prediction','sentiment_score'])\n",
        "  model.eval()\n",
        "  logits = model(tokens_tensor, token_type_ids=None,attention_mask=segments_tensors)\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  logits = softmax(logits)\n",
        "  sentiment_score = pd.Series(logits[:,0] - logits[:,1])\n",
        "  predictions = np.squeeze(np.argmax(logits, axis=1))\n",
        "  print(\"Text:\"+text)\n",
        "  print(\"Sentiment Score:\")\n",
        "  print(sentiment_score)\n",
        "  print(\"Predictions:['Negative:0','Positive:1']:\")\n",
        "  print(predictions)\n",
        "  print(\"Logits:['Negative','Positive']:\")\n",
        "  print(logits)\n",
        "  res = {\"text\":text,\"sentiment\":[predictions]}\n",
        "  df = pd.DataFrame(res)\n",
        "  df.to_csv('/content/drive/My Drive/'+filename+'.csv', mode='a', header=False)\n",
        "\n",
        " \n",
        "    \n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "x3bCIlmNntsZ",
        "outputId": "273d0d6c-c325-4766-b9ec-66931294bfd3"
      },
      "outputs": [],
      "source": [
        "line = \"I hate it when any athlete appears to tear an ACL on live television.\"\n",
        "keywordextract(line,\"sample\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "iNiKFjBMKWCN",
        "outputId": "3e3fb2b3-4fe1-481a-b94d-2bea0b708e90"
      },
      "outputs": [],
      "source": [
        "!pip install rake-nltk\n",
        "\n",
        "import operator\n",
        "\n",
        "from rake_nltk import Metric,Rake\n",
        "\n",
        "phrases = []\n",
        "def keyphrases_extract(text):\n",
        "  r = Rake(ranking_metric=Metric.WORD_DEGREE,min_length=1) \n",
        "  r.extract_keywords_from_text(text)\n",
        "  p_list = r.get_ranked_phrases()\n",
        "  p = {'text':text,'phrase':p_list}\n",
        "  pdf = pd.DataFrame(p)\n",
        "  pdf.to_csv('/content/drive/My Drive/phrases.csv', mode='a', header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1PynXppxQilH",
        "outputId": "bd19a000-366d-4e81-ab00-1047e22e45c9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import re\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from textblob import TextBlob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "% matplotlib inline\n",
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "auth = tweepy.AppAuthHandler('i2PBoMdhbQXqmIdAwXZKgawWQ', 'LYxmHirTinFxIMZbh1O3pkHPpIvo3MnLraCQijWx5N91n2sCAZ')\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
        "\t\t\t\t   wait_on_rate_limit_notify=True)\n",
        "\n",
        "if (not api):\n",
        "    print (\"Can't Authenticate\")\n",
        "    sys.exit(-1)\n",
        "\n",
        "#@title Twitter Search API Inputs\n",
        "#@markdown ### Enter Search Query:\n",
        "searchQuery = 'oneplus nord' #@param {type:\"string\"}\n",
        "\n",
        "maxTweets = 20\n",
        "Filter_Retweets = True #@param {type:\"boolean\"}\n",
        "\n",
        "tweetsPerQry = 100  # this is the max the API permits\n",
        "tweet_lst = []\n",
        "\n",
        "if Filter_Retweets:\n",
        "  searchQuery = searchQuery + ' -filter:retweets'  # to exclude retweets\n",
        "\n",
        "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
        "# else default to no lower limit, go as far back as API allows\n",
        "sinceId = None\n",
        "\n",
        "# If results only below a specific ID are, set max_id to that ID.\n",
        "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
        "max_id = -10000000000\n",
        "\n",
        "tweetCount = 0\n",
        "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
        "while tweetCount < maxTweets:\n",
        "    try:\n",
        "        if (max_id <= 0):\n",
        "            if (not sinceId):\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry, lang=\"en\")\n",
        "            else:\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", since_id=sinceId)\n",
        "        else:\n",
        "            if (not sinceId):\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", max_id=str(max_id - 1))\n",
        "            else:\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", max_id=str(max_id - 1),\n",
        "                                        since_id=sinceId)\n",
        "        if not new_tweets:\n",
        "            print(\"No more tweets found\")\n",
        "            break\n",
        "        for tweet in new_tweets:\n",
        "          if hasattr(tweet, 'reply_count'):\n",
        "            reply_count = tweet.reply_count\n",
        "          else:\n",
        "            reply_count = 0\n",
        "          if hasattr(tweet, 'retweeted'):\n",
        "            retweeted = tweet.retweeted\n",
        "          else:\n",
        "            retweeted = \"NA\"\n",
        "            \n",
        "          # fixup search query to get topic\n",
        "          topic = searchQuery[:searchQuery.find('-')].capitalize().strip()\n",
        "          \n",
        "          # fixup date\n",
        "          tweetDate = tweet.created_at.date()\n",
        "          \n",
        "          tweet_lst.append([tweetDate, topic, \n",
        "                      tweet.id, tweet.user.screen_name, tweet.user.name, tweet.text, tweet.favorite_count, \n",
        "                      reply_count, tweet.retweet_count, retweeted])\n",
        "\n",
        "        tweetCount += len(new_tweets)\n",
        "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
        "        max_id = new_tweets[-1].id\n",
        "    except tweepy.TweepError as e:\n",
        "        # Just exit if any error\n",
        "        print(\"some error : \" + str(e))\n",
        "        break\n",
        "\n",
        "clear_output()\n",
        "print(\"Downloaded {0} tweets\".format(tweetCount))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Z-bfHvXJSYju",
        "outputId": "ff31172c-b623-42b1-9abc-f801035fc69c"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "# load it into a pandas dataframe\n",
        "tweet_df = pd.DataFrame(tweet_lst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'tweet', 'like_count', 'reply_count', 'retweet_count', 'retweeted'])\n",
        "tweet_df.head()\n",
        "tweet_df.to_csv('/content/drive/My Drive/opntwweets.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CtrhGL3XSmjJ",
        "outputId": "4b4c4977-f5a2-48bf-c5d5-4a73da77ffa4"
      },
      "outputs": [],
      "source": [
        "for index, row in tqdm(tweet_df.iterrows(), total=tweet_df.shape[0]):\n",
        "  cleanedTweet = row['tweet'].replace(\"#\", \"\")\n",
        "  keywordextract(cleanedTweet,searchQuery)\n",
        "  keyphrases_extract(cleanedTweet)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKwhPKRxWuCC"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install wordcloud\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd \n",
        "\n",
        "comment_words = ' '\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "phrase_df=pd.read_csv('/content/drive/My Drive/phrases.csv',encoding='utf-8',low_memory=False,index_col=1)\n",
        "phrase_df.columns = ['phrase']\n",
        "phrase_df = phrase_df.groupby('phrase')\n",
        "print(phrase_df)\n",
        "phrase_list = phrase_df\n",
        "for val in phrase_list: \n",
        "      \n",
        "    # typecaste each val to string \n",
        "    val = str(val) \n",
        "  \n",
        "    # split the value \n",
        "    tokens = val.split() \n",
        "      \n",
        "    # Converts each token into lowercase \n",
        "    for i in range(len(tokens)): \n",
        "        tokens[i] = tokens[i].lower() \n",
        "          \n",
        "    for words in tokens: \n",
        "      comment_words = comment_words + words + ' '\n",
        "wordcloud = WordCloud(width = 800, height = 800, \n",
        "                background_color ='white', \n",
        "                stopwords = stopwords, \n",
        "                min_font_size = 6).generate(comment_words) \n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "wordcloud.to_file('/content/drive/My Drive/wordcloud.png')\n",
        "files.download('/content/drive/My Drive/wordcloud.png')\n",
        "# plot the WordCloud image                        \n",
        "plt.figure(figsize = (6, 6), facecolor = None) \n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "_ugMz0MGUNpn",
        "outputId": "a8b88a43-474e-44ff-be2b-7ab81518a949"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "\n",
        "sent_df=pd.read_csv('/content/drive/My Drive/'+searchQuery+'.csv',encoding='utf-8',low_memory=False)\n",
        "sent_df.columns = ['rand','tweet','sent']\n",
        "\n",
        "x = sent_df.tweet.values\n",
        "y = sent_df.sent.values\n",
        "\n",
        "pos_tweets = [ tweet for index, tweet in enumerate(sent_df['tweet']) if sent_df['sent'][index] == 1]\n",
        "neg_tweets = [ tweet for index, tweet in enumerate(sent_df['tweet']) if sent_df['sent'][index] == 0]\n",
        "\n",
        "print(\"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(sent_df['tweet'])))\n",
        "print(\"Percentage of negative tweets: {}%\".format(len(neg_tweets)*100/len(sent_df['tweet'])))\n",
        "\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "sentiment_summary = dict(pos=0,neg=0,neu=0)\n",
        "\n",
        "for sentence in x:\n",
        "  sentiment_score = sid.polarity_scores(sentence)\n",
        "  if sentiment_score[\"compound\"] == 0.0:\n",
        "            sentiment_summary[\"neu\"] += 1\n",
        "  elif sentiment_score[\"compound\"] > 0.0:\n",
        "            sentiment_summary[\"pos\"] += 1\n",
        "  else:\n",
        "            sentiment_summary[\"neg\"] += 1\n",
        "\n",
        "x=('positive','negative','neutral')\n",
        "y_pos = np.arange(len(x))\n",
        "y=[sentiment_summary['pos'],sentiment_summary['neg'],sentiment_summary['neu']]\n",
        "\n",
        "print(sentiment_summary)\n",
        "\n",
        "plt.bar(x, y, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos,x)\n",
        "plt.ylabel('sentiment balance')\n",
        "plt.title('Sentiment Balance')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "9e751d02d6254c91a5337ccacd3da0a06bef2f97f2b707c83ebd4c7968b8ad4d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
